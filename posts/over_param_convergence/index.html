<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Convergence of gradient descent in over-parameterized networks | Mina Ghashami's blog</title><meta name=keywords content><meta name=description content="Neural networks typically have very large number of parameters. Depending on whether they have more parameters than training instances, they are over-parameterized or under-parameterized. In either case, their loss function is a multivariable, multidimensional and often non-convex function. In this post, we study over-parameterized neural networks and their loss landscape; we answer the question of why gradient descent (GD) and its variants converge to global minima in over-parameterized neural networks, even though their loss function is non-convex."><meta name=author content="Mina Ghashami"><link rel=canonical href=https://mina-ghashami.github.io/posts/over_param_convergence/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.74d6d4f35a66db3737233888dcc149bb2a72ab40c9ff8c16d6f12e3bafe9a871.css integrity="sha256-dNbU81pm2zc3IziI3MFJuypyq0DJ/4wW1vEuO6/pqHE=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://mina-ghashami.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://mina-ghashami.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://mina-ghashami.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://mina-ghashami.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://mina-ghashami.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Convergence of gradient descent in over-parameterized networks"><meta property="og:description" content="Neural networks typically have very large number of parameters. Depending on whether they have more parameters than training instances, they are over-parameterized or under-parameterized. In either case, their loss function is a multivariable, multidimensional and often non-convex function. In this post, we study over-parameterized neural networks and their loss landscape; we answer the question of why gradient descent (GD) and its variants converge to global minima in over-parameterized neural networks, even though their loss function is non-convex."><meta property="og:type" content="article"><meta property="og:url" content="https://mina-ghashami.github.io/posts/over_param_convergence/"><meta property="og:image" content="https://mina-ghashami.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-09-16T23:00:33-07:00"><meta property="article:modified_time" content="2022-09-16T23:00:33-07:00"><meta property="og:site_name" content="Mina Ghashami's blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://mina-ghashami.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Convergence of gradient descent in over-parameterized networks"><meta name=twitter:description content="Neural networks typically have very large number of parameters. Depending on whether they have more parameters than training instances, they are over-parameterized or under-parameterized. In either case, their loss function is a multivariable, multidimensional and often non-convex function. In this post, we study over-parameterized neural networks and their loss landscape; we answer the question of why gradient descent (GD) and its variants converge to global minima in over-parameterized neural networks, even though their loss function is non-convex."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://mina-ghashami.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Convergence of gradient descent in over-parameterized networks","item":"https://mina-ghashami.github.io/posts/over_param_convergence/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Convergence of gradient descent in over-parameterized networks","name":"Convergence of gradient descent in over-parameterized networks","description":"Neural networks typically have very large number of parameters. Depending on whether they have more parameters than training instances, they are over-parameterized or under-parameterized. In either case, their loss function is a multivariable, multidimensional and often non-convex function. In this post, we study over-parameterized neural networks and their loss landscape; we answer the question of why gradient descent (GD) and its variants converge to global minima in over-parameterized neural networks, even though their loss function is non-convex.","keywords":[],"articleBody":"Neural networks typically have very large number of parameters. Depending on whether they have more parameters than training instances, they are over-parameterized or under-parameterized. In either case, their loss function is a multivariable, multidimensional and often non-convex function. In this post, we study over-parameterized neural networks and their loss landscape; we answer the question of why gradient descent (GD) and its variants converge to global minima in over-parameterized neural networks, even though their loss function is non-convex.\nNotation and problem definition Let us consider a training dataset $D=\\{x_i, y_i\\}_{i=1}^n$ with $x_i\\in \\mathbb{R}^d$, $y \\in \\mathbb{R}$, and a parametric family of models $f(w;x)$ e.g. neural networks where $w \\in \\mathbb{R}^m$. We wish to find a model with parameter $w^*$ such that it fits the training data, i.e. $$f(w^*; x_i)\\approx y_i, \\forall i\\in[1,n]$$ Since $x$ is not a model parameter, we can compact this notation and write it as: \\begin{aligned} \u0026F(w)=y\\\\ \u0026F:\\mathbb{R}^m \\rightarrow \\mathbb{R}^n \\\\ \u0026 w\\in \\mathbb{R}^m, y\\in \\mathbb{R}^n \\end{aligned} Where $(F(w))_i = F(w; x_i)$. To minimize this system and find optimal parameter $w^*$ a certain loss function $L(w)$ is defined, e.g., the least square loss $L(w) = \\frac{1}{2}\\|F(w)-y\\|^2$. Irrespective of the choice of loss function, $L(w)$ will be non-convex as $F$ in neural network contains non-linear activations and is non-convex.\nFor vectors, we use $\\|.\\|$ to denote Euclidean norm. For a function $F:\\mathbb{R}^m \\rightarrow \\mathbb{R}^n$, we use $\\nabla F(w)$ to denote gradient of $F$ with $(\\nabla F(w))_{i,j} = \\frac{\\partial F_i}{\\partial w_j}$. We denote Hessian of $F$ as $H_F \\in \\mathbb{R}^{n \\times m \\times m}$ with $(H_F)_{ijk} = \\frac{\\partial^2 F_i}{\\partial w_j \\partial w_k}$. We denote Hessian of the loss function as $H_L$ which is an $m \\times m$ matrix. We denote neural tangent kernel of $F$ as $K(w) = K(w; x,x’) = \\nabla F(w;x) \\nabla F(w;x’)^T$.\nWe denote eigenvalues of a function/matrix $f$ with $\\lambda(f)$, where $\\lambda_{max}(f),\\lambda_{min}(f)$ are largest and smallest eigenvalues of $f$, respectively.\nSolving this system in over-parameterized setting i.e. where $m \u003e n$ is the focus of this post; we show exact solutions exist.\nLoss landscape We refer to the representation of loss function with respect to model parameters as loss landscape. We know loss landscape of neural networks is highly non-convex; but there is a key difference between loss landscape of under-parameterized networks and that of over-parameterized networks.\nUnder-parameterized NNs have a globally non-convex loss landscape, however they are locally convex in a sufficiently small neighborhood of local minima. On the other hand, loss landscape of over-parameterized systems is essentially non-convex, meaning that they are globally as well as locally non-convex; i.e. in any arbitrarily small neighborhood around global minimum they are non-convex. As the result, we can not use convexity based optimization theorems for analyzing over-parameterized NNs.\nNon-isolated local/global minima For under-parameterized systems, local minima are generally isolated, however in over-parameterized networks, there is no isolated local/global minima in the loss landscape. Chaoyue Liu et. al prove the phenomena in appendix A of their paper, but to see it intuitively consider the following example:\nExample: Consider an over-parameterized system with two training instances $\\{(x_1,y_1), (x_2,y_2)\\}$ where $x_i\\in \\mathbb{R}^2$, $y_i \\in \\mathbb{R}$, and a parameter space of $w\\in \\mathbb{R}^3$. To minimize the training loss, we have to find $w^*=(w^0, w^1, w^2)$ such that the following two equations hold: $$ w^0 + w^1 x_1^1 + w^2 x_1^2 = y_1\\\\ w^0 + w^1 x_2^1 + w^2 x_2^2 = y_2 $$ Above equations are planes in $\\mathbb{R}^3$, and their intersection which is a line is the set of global minima $w^*$; since this set is a line it is a non-isolated manifold.\nNon-convexity of manifold of global minima We saw that $w^*$s are non-isolated and form a manifold, here we show the manifold of global minima is non-convex. Let $M$ denote manifold of $w^*$, where $\\forall w \\in M, w$ is a global minima of $L(w)$. If the manifold was convex, then for two $w_1, w_2 \\in M$, the in-between points $v = \\alpha w_1 + (1-\\alpha)w_2$ for $\\alpha \\in (0,1)$ will have a lower loss i.e. $L(v) \\le L(w_1)$. But we already know $M$ is the minizer of loss function, so the manifold can not be convex.\nPolyak-Lojasiewicz (PL) condition In absence of convex based optimization methods for over-parameterized networks, a new framework based on Polyak-Lojasiewicz (PL) condition helps us to analyze them.\n$\\mu$-PL condition. A non-negative loss function $L$ satisfies $\\mu$-PL condition for $\\mu \u003e 0$ if, $$ \\|\\nabla L(w) \\|^2 \\ge \\mu L(w), \\forall w \\in B $$ Where $B = B(w_0, r) = \\{w|\\text{ }\\|w-w_0\\| \\le r \\}$ is a neighborhood around initial value $w_0$ with radius $r$.\nWe will show that if $\\mu$-PL condition is satisfied, then global minima $w^*$ exists and GD converges to $w^*$ exponentially fast.\nConvergence under PL condition Specifically we show under three conditioning assumptions:\n$L(w)$ is $\\mu$-PL for $\\forall w \\in B$ $L(w)$ is $\\beta$-smooth, i.e. $\\lambda_{max}(H) \\le \\beta$ GD’s learning rate is small enough i.e. $\\eta = 1/\\beta$ GD will converge to global minima with an exponentially fast convergence rate.\nproof. Gradient descent’s update rule is $w_{t+1} = w_t - \\eta \\nabla L(w_t)$. Using Taylor’s expansion we will have: \\begin{aligned} L(w_{t+1}) \u0026= L(w_t) + \\color{blue}{\\underbrace{(w_{t+1} - w_t)^T}_{-\\eta\\nabla L(w_t)^T}} \\nabla L(w_t) + \\frac{1}{2} \\color{blue}{\\underbrace{(w_{t+1} - w_t)^T}_{-\\eta\\nabla L(w_t)^T}} H(w_t) \\color{blue}{\\underbrace{(w_{t+1} - w_t)}_{-\\eta\\nabla L(w_t)}} \\\\ \u0026\\leq L(w_t) - \\eta \\|\\nabla L(w_t) \\|^2 + \\frac{\\eta^2}{2} \\color{blue}{\\underbrace{\\nabla L(w_t)^T H(w) \\nabla L(w_t)}_{\\leq \\beta \\|\\nabla L(w_t)\\|^2}} \\\\ \u0026\\leq L(w_t) - \\eta \\|\\nabla L(w_t)\\|^2 (1-\\frac{\\eta\\beta}{2}) \\\\ \u0026\\leq L(w_t) - \\frac{\\eta}{2} \\color{blue}{\\underbrace{\\|\\nabla L(w_t)\\|^2}_{\\geq \\mu L(w_t)}} \\\\ \u0026\\leq (1-\\eta \\mu)L(w_t) \\end{aligned} Therefore after $t$ steps of gradient descent, $L(w_{t+1}) \\leq (1-\\eta \\mu)^t L(w_0)$. This shows loss decays exponentially fast with the decay rate of $(1-\\eta \\mu)$.\nOver-parameterized NNs satisfy PL condition While the second and third condition in convergence proof are commonly used in convex cases as well, it is not obvious why first condition holds for over-parameterized NNs. In this section, we intuitively show why $\\mu-PL$ condition holds for these networks.\nFirst we note that in over-parameterized NNs, the least eigenvalue of NTK is separated from zero, i.e. $\\lambda_{min}(K(w)) \u003e 0$. The reason for this is that \\begin{aligned} \\text{rank}(K(w)) \u0026= \\text{rank}(\\nabla F(w) \\nabla F(w)^T) \\\\ \u0026=\\text{rank}(F(w)) \\\\ \u0026= \\min(m,n) \\\\ \u0026=n \\end{aligned} And since $K(w)\\in \\mathbb{R}^{n\\times n}$ it can not be degenerate i.e. does not have zero eigenvalues. So we can always assume $\\lambda_{min}(K(w)) \\ge \\mu$ for a positive $\\mu$.\nLemma. If $\\lambda_{min}(K(w)) \\ge \\mu$ for all $w\\in B$ then, the square loss function $L(w) = \\frac{1}{2}\\|F(w)-y\\|^2$ is $\\mu$-PL on $w\\in B$.\nproof.\n\\begin{aligned} \\frac{1}{2}\\|\\nabla L(w)\\|^2 \u0026= \\frac{1}{2}\\| (F(w)-y)^T \\nabla F(w) \\|^2 \\\\ \u0026=\\frac{1}{2} (F(w)-y)^T \\nabla F(w) \\nabla F(w)^T (F(w)-y) \\\\ \u0026= \\frac{1}{2} (F(w)-y)^T K(w) (F(w)-y) \\\\ \u0026\\ge \\frac{1}{2} \\lambda_{min}(K(w)) \\|F(w)-y\\|^2 \\\\ \u0026= \\lambda_{min}(K(w)) L(w) \\end{aligned} and therefore it satisfies PL condition.\nI would like to finish this post by noting that $\\eta \\mu$ in the convergence rate, is actually the inverse of condition number of the NN function: $$ \\text{Condition number}(F) = K_F = \\frac{\\sup_B \\lambda_{max} (H)}{\\inf_B \\lambda_{min}(K)} $$ Using notation of above section, we see that $K_F = \\frac{\\beta}{\\mu} = \\frac{1}{\\eta \\mu}$. The smaller condition number is the better it is as it leads to faster convergence of GD. This paper proves that overparamterization helps with condition number meaning as number of parameters $m \\rightarrow \\infty$ , the condition number $K_F \\rightarrow 1$.\nReferences Simon S. Du et. al GRADIENT DESCENT PROVABLY OPTIMIZES OVER-PARAMETERIZED NEURAL NETWORKS Chaoyue Liu et. al Loss landscapes and optimization in over-parameterized non-linear systems and neural networks ","wordCount":"1235","inLanguage":"en","datePublished":"2022-09-16T23:00:33-07:00","dateModified":"2022-09-16T23:00:33-07:00","author":{"@type":"Person","name":"Mina Ghashami"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mina-ghashami.github.io/posts/over_param_convergence/"},"publisher":{"@type":"Organization","name":"Mina Ghashami's blog","logo":{"@type":"ImageObject","url":"https://mina-ghashami.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mina-ghashami.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://mina-ghashami.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mina-ghashami.github.io/posts/ title=posts><span>posts</span></a></li><li><a href=https://mina-ghashami.github.io/aboutme/ title="about me"><span>about me</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Convergence of gradient descent in over-parameterized networks</h1><div class=post-meta><span title='2022-09-16 23:00:33 -0700 PDT'>September 2022</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1235 words&nbsp;·&nbsp;Mina Ghashami&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/over_param_convergence.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#notation-and-problem-definition>Notation and problem definition</a></li><li><a href=#loss-landscape>Loss landscape</a><ul><li><a href=#non-isolated-localglobal-minima>Non-isolated local/global minima</a></li><li><a href=#non-convexity-of-manifold-of-global-minima>Non-convexity of manifold of global minima</a></li></ul></li><li><a href=#polyak-lojasiewicz-pl-condition>Polyak-Lojasiewicz (PL) condition</a><ul><li><a href=#convergence-under-pl-condition>Convergence under PL condition</a></li><li><a href=#over-parameterized-nns-satisfy-pl-condition>Over-parameterized NNs satisfy PL condition</a></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><p>Neural networks typically have very large number of parameters. Depending on whether they have more parameters than training instances,
they are over-parameterized or under-parameterized. In either case, their loss function is a multivariable, multidimensional and often non-convex function.
In this post, we study over-parameterized neural networks and their loss landscape; we answer the question of why gradient descent (GD) and its variants converge
to global minima in over-parameterized neural networks, even though their loss function is non-convex.</p><h2 id=notation-and-problem-definition>Notation and problem definition<a hidden class=anchor aria-hidden=true href=#notation-and-problem-definition>#</a></h2><p>Let us consider a training dataset $D=\{x_i, y_i\}_{i=1}^n$ with $x_i\in \mathbb{R}^d$, $y \in \mathbb{R}$, and a parametric family of models $f(w;x)$ e.g. neural
networks where $w \in \mathbb{R}^m$. We wish to find a model with parameter $w^*$ such that it fits the training data, i.e. $$f(w^*; x_i)\approx y_i, \forall i\in[1,n]$$
Since $x$ is not a model parameter, we can compact this notation and write it as:
\begin{aligned}
&F(w)=y\\
&F:\mathbb{R}^m \rightarrow \mathbb{R}^n \\
& w\in \mathbb{R}^m, y\in \mathbb{R}^n
\end{aligned}
Where $(F(w))_i = F(w; x_i)$. To minimize this system and find optimal parameter $w^*$ a certain loss function $L(w)$ is defined, e.g.,
the least square loss $L(w) = \frac{1}{2}\|F(w)-y\|^2$. Irrespective of the choice of loss function, $L(w)$ will be non-convex as $F$ in neural network contains
non-linear activations and is non-convex.</p><p>For vectors, we use $\|.\|$ to denote Euclidean norm. For a function $F:\mathbb{R}^m \rightarrow \mathbb{R}^n$,
we use $\nabla F(w)$ to denote gradient of $F$ with $(\nabla F(w))_{i,j} = \frac{\partial F_i}{\partial w_j}$. We denote Hessian
of $F$ as $H_F \in \mathbb{R}^{n \times m \times m}$ with $(H_F)_{ijk} = \frac{\partial^2 F_i}{\partial w_j \partial w_k}$.
We denote Hessian of the loss function as $H_L$ which is an $m \times m$ matrix.
We denote neural tangent kernel of $F$ as $K(w) = K(w; x,x&rsquo;) = \nabla F(w;x) \nabla F(w;x&rsquo;)^T$.</p><p>We denote eigenvalues of a function/matrix $f$ with $\lambda(f)$, where $\lambda_{max}(f),\lambda_{min}(f)$ are largest and smallest eigenvalues of $f$, respectively.</p><p>Solving this system in over-parameterized setting i.e. where $m > n$ is the focus of this post; we show exact solutions exist.</p><h2 id=loss-landscape>Loss landscape<a hidden class=anchor aria-hidden=true href=#loss-landscape>#</a></h2><p>We refer to the representation of loss function with respect to model parameters as loss landscape. We know loss landscape of neural networks is highly non-convex;
but there is a key difference between loss landscape of under-parameterized networks and that of over-parameterized networks.</p><p>Under-parameterized NNs have a globally non-convex loss landscape, however they are locally convex in a sufficiently small neighborhood of local minima.
On the other hand, loss landscape of over-parameterized systems is <em>essentially non-convex</em>,
meaning that they are globally as well as locally non-convex; i.e. in any arbitrarily small neighborhood around global minimum they are non-convex. As the result,
we can not use convexity based optimization theorems for analyzing over-parameterized NNs.</p><h3 id=non-isolated-localglobal-minima>Non-isolated local/global minima<a hidden class=anchor aria-hidden=true href=#non-isolated-localglobal-minima>#</a></h3><p>For under-parameterized systems, local minima are generally isolated, however in over-parameterized networks, there is no isolated local/global minima in the
loss landscape. <a href=https://arxiv.org/abs/2003.00307>Chaoyue Liu et. al</a> prove the phenomena in appendix A of their paper, but to see it intuitively consider the following example:</p><p>Example: Consider an over-parameterized system with two training instances $\{(x_1,y_1), (x_2,y_2)\}$ where $x_i\in \mathbb{R}^2$, $y_i \in \mathbb{R}$,
and a parameter space of $w\in \mathbb{R}^3$. To minimize the training loss, we have to find $w^*=(w^0, w^1, w^2)$ such that the following two equations hold:
$$
w^0 + w^1 x_1^1 + w^2 x_1^2 = y_1\\
w^0 + w^1 x_2^1 + w^2 x_2^2 = y_2
$$
Above equations are planes in $\mathbb{R}^3$, and their intersection which is a line is the set of global minima $w^*$; since this set is a line it
is a non-isolated manifold.</p><h3 id=non-convexity-of-manifold-of-global-minima>Non-convexity of manifold of global minima<a hidden class=anchor aria-hidden=true href=#non-convexity-of-manifold-of-global-minima>#</a></h3><p>We saw that $w^*$s are non-isolated and form a manifold, here we show the manifold of global minima is non-convex.
Let $M$ denote manifold of $w^*$, where $\forall w \in M, w$ is a global minima of $L(w)$. If the manifold was convex, then for two $w_1, w_2 \in M$,
the in-between points $v = \alpha w_1 + (1-\alpha)w_2$ for $\alpha \in (0,1)$ will have a lower loss i.e. $L(v) \le L(w_1)$. But we already know $M$ is the minizer
of loss function, so the manifold can not be convex.</p><h2 id=polyak-lojasiewicz-pl-condition>Polyak-Lojasiewicz (PL) condition<a hidden class=anchor aria-hidden=true href=#polyak-lojasiewicz-pl-condition>#</a></h2><p>In absence of convex based optimization methods for over-parameterized networks,
a new framework based on Polyak-Lojasiewicz (PL) condition helps us to analyze them.</p><p><strong>$\mu$-PL condition</strong>. A non-negative loss function $L$ satisfies $\mu$-PL condition for $\mu > 0$ if,
$$
\|\nabla L(w) \|^2 \ge \mu L(w), \forall w \in B
$$
Where $B = B(w_0, r) = \{w|\text{ }\|w-w_0\| \le r \}$ is a neighborhood around initial value $w_0$ with radius $r$.</p><p>We will show that if $\mu$-PL condition is satisfied, then global minima $w^*$ exists and GD converges to $w^*$ exponentially fast.</p><h3 id=convergence-under-pl-condition>Convergence under PL condition<a hidden class=anchor aria-hidden=true href=#convergence-under-pl-condition>#</a></h3><p>Specifically we show under three conditioning assumptions:</p><ol><li>$L(w)$ is $\mu$-PL for $\forall w \in B$</li><li>$L(w)$ is $\beta$-smooth, i.e. $\lambda_{max}(H) \le \beta$</li><li>GD&rsquo;s learning rate is small enough i.e. $\eta = 1/\beta$</li></ol><p>GD will converge to global minima with an exponentially fast convergence rate.</p><p><strong>proof.</strong>
Gradient descent&rsquo;s update rule is $w_{t+1} = w_t - \eta \nabla L(w_t)$. Using Taylor&rsquo;s expansion we will have:
\begin{aligned}
L(w_{t+1}) &= L(w_t) + \color{blue}{\underbrace{(w_{t+1} - w_t)^T}_{-\eta\nabla L(w_t)^T}} \nabla L(w_t) + \frac{1}{2} \color{blue}{\underbrace{(w_{t+1} - w_t)^T}_{-\eta\nabla L(w_t)^T}} H(w_t) \color{blue}{\underbrace{(w_{t+1} - w_t)}_{-\eta\nabla L(w_t)}} \\
&\leq L(w_t) - \eta \|\nabla L(w_t) \|^2 + \frac{\eta^2}{2} \color{blue}{\underbrace{\nabla L(w_t)^T H(w) \nabla L(w_t)}_{\leq \beta \|\nabla L(w_t)\|^2}} \\
&\leq L(w_t) - \eta \|\nabla L(w_t)\|^2 (1-\frac{\eta\beta}{2}) \\
&\leq L(w_t) - \frac{\eta}{2} \color{blue}{\underbrace{\|\nabla L(w_t)\|^2}_{\geq \mu L(w_t)}} \\
&\leq (1-\eta \mu)L(w_t)
\end{aligned}
Therefore after $t$ steps of gradient descent, $L(w_{t+1}) \leq (1-\eta \mu)^t L(w_0)$. This shows loss decays exponentially fast with the decay rate of
$(1-\eta \mu)$.</p><h3 id=over-parameterized-nns-satisfy-pl-condition>Over-parameterized NNs satisfy PL condition<a hidden class=anchor aria-hidden=true href=#over-parameterized-nns-satisfy-pl-condition>#</a></h3><p>While the second and third condition in convergence proof are commonly used in convex cases as well, it is not obvious why first condition
holds for over-parameterized NNs. In this section, we intuitively show why $\mu-PL$ condition holds for these networks.</p><p>First we note that in over-parameterized NNs, the least eigenvalue of NTK is separated from zero, i.e. $\lambda_{min}(K(w)) > 0$. The reason for this is that
\begin{aligned}
\text{rank}(K(w)) &= \text{rank}(\nabla F(w) \nabla F(w)^T) \\
&=\text{rank}(F(w)) \\
&= \min(m,n) \\
&=n
\end{aligned}
And since $K(w)\in \mathbb{R}^{n\times n}$ it can not be degenerate i.e. does not have zero eigenvalues.
So we can always assume $\lambda_{min}(K(w)) \ge \mu$ for a positive $\mu$.</p><p><strong>Lemma.</strong> If $\lambda_{min}(K(w)) \ge \mu$ for all $w\in B$ then, the square loss function $L(w) = \frac{1}{2}\|F(w)-y\|^2$ is $\mu$-PL on $w\in B$.</p><p><strong>proof.</strong></p><p>\begin{aligned}
\frac{1}{2}\|\nabla L(w)\|^2 &= \frac{1}{2}\| (F(w)-y)^T \nabla F(w) \|^2 \\
&=\frac{1}{2} (F(w)-y)^T \nabla F(w) \nabla F(w)^T (F(w)-y) \\
&= \frac{1}{2} (F(w)-y)^T K(w) (F(w)-y) \\
&\ge \frac{1}{2} \lambda_{min}(K(w)) \|F(w)-y\|^2 \\
&= \lambda_{min}(K(w)) L(w)
\end{aligned}
and therefore it satisfies PL condition.</p><p>I would like to finish this post by noting that $\eta \mu$ in the convergence rate, is actually the inverse of condition number of the NN function:
$$
\text{Condition number}(F) = K_F = \frac{\sup_B \lambda_{max} (H)}{\inf_B \lambda_{min}(K)}
$$
Using notation of above section, we see that $K_F = \frac{\beta}{\mu} = \frac{1}{\eta \mu}$. The smaller condition number is the better it is
as it leads to faster convergence of GD. <a href=https://arxiv.org/abs/1810.02054>This paper</a> proves that overparamterization
helps with condition number meaning as number of parameters $m \rightarrow \infty$
, the condition number $K_F \rightarrow 1$.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li>Simon S. Du et. al <a href=https://arxiv.org/abs/1810.02054>GRADIENT DESCENT PROVABLY OPTIMIZES OVER-PARAMETERIZED NEURAL NETWORKS</a></li><li>Chaoyue Liu et. al <a href=https://arxiv.org/abs/2003.00307>Loss landscapes and optimization in over-parameterized non-linear systems and neural networks</a></li></ol></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://mina-ghashami.github.io/>Mina Ghashami's blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>