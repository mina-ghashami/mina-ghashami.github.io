<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Transformer: Concept and code from scratch | Mina Ghashami's blog</title><meta name=keywords content><meta name=description content="Transformers are novel neural networks that are mainly used for sequence transduction tasks. Sequence transduction is any task where input sequences are transformed into output sequences. Most competitive neural sequence transduction models have an encoder-decoder structure. The encoder maps an input sequence of symbol representations to a sequence of continuous representations, the decoder then generates an output sequence of symbols one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next."><meta name=author content="Mina Ghashami"><link rel=canonical href=https://mina-ghashami.github.io/posts/2023-01-10-transformer/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.74d6d4f35a66db3737233888dcc149bb2a72ab40c9ff8c16d6f12e3bafe9a871.css integrity="sha256-dNbU81pm2zc3IziI3MFJuypyq0DJ/4wW1vEuO6/pqHE=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://mina-ghashami.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://mina-ghashami.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://mina-ghashami.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://mina-ghashami.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://mina-ghashami.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Transformer: Concept and code from scratch"><meta property="og:description" content="Transformers are novel neural networks that are mainly used for sequence transduction tasks. Sequence transduction is any task where input sequences are transformed into output sequences. Most competitive neural sequence transduction models have an encoder-decoder structure. The encoder maps an input sequence of symbol representations to a sequence of continuous representations, the decoder then generates an output sequence of symbols one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next."><meta property="og:type" content="article"><meta property="og:url" content="https://mina-ghashami.github.io/posts/2023-01-10-transformer/"><meta property="og:image" content="https://mina-ghashami.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-01-05T11:18:07-08:00"><meta property="article:modified_time" content="2023-01-05T11:18:07-08:00"><meta property="og:site_name" content="Mina Ghashami's blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://mina-ghashami.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Transformer: Concept and code from scratch"><meta name=twitter:description content="Transformers are novel neural networks that are mainly used for sequence transduction tasks. Sequence transduction is any task where input sequences are transformed into output sequences. Most competitive neural sequence transduction models have an encoder-decoder structure. The encoder maps an input sequence of symbol representations to a sequence of continuous representations, the decoder then generates an output sequence of symbols one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"","item":"https://mina-ghashami.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Transformer: Concept and code from scratch","item":"https://mina-ghashami.github.io/posts/2023-01-10-transformer/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Transformer: Concept and code from scratch","name":"Transformer: Concept and code from scratch","description":"Transformers are novel neural networks that are mainly used for sequence transduction tasks. Sequence transduction is any task where input sequences are transformed into output sequences. Most competitive neural sequence transduction models have an encoder-decoder structure. The encoder maps an input sequence of symbol representations to a sequence of continuous representations, the decoder then generates an output sequence of symbols one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.","keywords":[],"articleBody":"Transformers are novel neural networks that are mainly used for sequence transduction tasks. Sequence transduction is any task where input sequences are transformed into output sequences. Most competitive neural sequence transduction models have an encoder-decoder structure. The encoder maps an input sequence of symbol representations to a sequence of continuous representations, the decoder then generates an output sequence of symbols one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next. In this post, I’ll document my learnings on main building blocks of transformer and how to implement them using PyTorch.\nimport copy import torch import math import torch.nn as nn from torch.nn.functional import log_softmax, pad class EncoderDecoder(nn.Module): \"\"\" A standard Encoder-Decoder architecture. \"\"\" def __init__(self, encoder, decoder, src_embed, tgt_embed, generator): super(EncoderDecoder, self).__init__() self.encoder = encoder self.decoder = decoder self.src_embed = src_embed self.tgt_embed = tgt_embed self.generator = generator def forward(self, src, tgt, src_mask, tgt_mask): # Take in and process masked src and target sequences. return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask) def encode(self, src, src_mask): # Pass input sequence i.e. src through encoder return self.encoder(self.src_embed(src), src_mask) def decode(self, memory, src_mask, tgt, tgt_mask): # Memory is the query and key from encoder return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask) class Generator(nn.Module): # Define standard linear + softmax generation step. def __init__(self, d_model, vocab): super(Generator, self).__init__() self.proj = nn.Linear(d_model, vocab) def forward(self, x): return log_softmax(self.proj(x), dim=-1) Encoder Encoder’s job is to convert an input sequence of tokens into a contextualized sequence of embedding vectors. By contextualized we mean the embedding of each token contains information about the context of the sequence such that the word “apple” as a fruit have a different embedding than “apple” as a company.\nThe encoder block is demonstrated in the figure below; it consists of multiple (historically 6) encoder layers. First encoder layer, takes the input embedding and add positional embeddings to it. Positional embeddings is shown as $\\sim$ symbol in the image, and we’ll explain it soon. This only happens in the first encoder layer. All encoder layers has two sub-layers: a multi-head self-attention mechanism, and a simple, position-wise fully connected feed-forward network. Each encoder layer receives a sequence of embeddings and feeds them through the two sublayers. The output embedding of an encoder layer has the same size as its inputs, and it is passed to the next encoder layer. Eventually all encoder layers together update the input embeddings so that they contain some contextual information from the sequence.\nEach encoder layer uses skip connections, shown with cyan arrows in the image below, and layer normalization, shows as “add \u0026 norm” box in the image below. Before, we dive into the encoder code, we explain each of these components.\nSkip connection also known as residual connection pass a tensor to the next layer without processing and add it to the processed tensor. Skip connection is a simple yet very effective technique to mitigate the vanishing gradient problem in training deep neural networks and help them converge faster. They were first introduced in ResNet in computer vision. Layer normalization which is shown as “add \u0026 norm” box normalizes each input in the batch to have zero mean and unit variance. In literature, there are two ways to place “Add \u0026 Norm”: Post layer normalization as shown in this figure where normalization happens after attention, and pre layer norm where “Add \u0026 Norm” is placed before multi-head attention. The former was used in the original transformer paper and is tricky to train from scratch as the gradients diverge. The latter however is more stable during training and is most commonly used.\nPositional Embedding In addition to embed semantic of a token, we need to embed token’s position in the sequence too. This is because transformer processes input tokens in parallel and not sequentially; as a result it does not carry any information on positions of tokens, so we need to embed this information into the input. Positional embeddings are identifiers that are added to tokens’ embeddings, and must satisfy two requirements:\nIt should be the same for a position irrespective of the token in that position. So while the sequence might change, the positional embeddings must stay the same. They should not be too large, or otherwise they will dominate semantic similarity. Based on above criteria, we can not use a non-periodic function (e.g. linear) that numbers tokens from 1 to total number of tokens. Such functions violate the second requirement above. A working option would be to use sine and cosine functions. Sine and cosine periodically return a number in $(-1,1)$ range and are bounded. They are defined everywhere, and so even on very large sequences all tokens receive a positional embedding. Moreover, they have large variability even for large numbers. This is in contrast to sigmoid function which gets flat on large values. The disadvantage of sine/cosine is that it repeats same outcome for many positions. This is not desirable, so we can give our function a low frequency (so that sine/cosine is stretched) such that even for our biggest sequence length the numbers do not repeat. This has the benefit of the linear function while positions are bounded BUT it has the cons of having little difference between consecutive positions. We want different positions to have meaningful differences. So we use a low-frequency sine for first dimension of positional embeddings. For the second dimension, we use cosine with a higher frequency.\nWe repeat this for all dimensions: alternate between sine and cosine and increase frequency. In math, $PE_{(pos, 2i)} = \\sin(pos/10000^{2i/d_{model}})$ and $PE_{(pos, 2i+1)} = \\cos(pos/10000^{2i/d_{model}})$ for position $pos$ and dimension $i$.\nclass PositionalEncoding(nn.Module): # Implement the position encoding (PE) function. def __init__(self, d_model, dropout, max_len=5000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) # Compute the positional encodings once in log space. pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1) div_term = torch.exp( torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model) ) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0) self.register_buffer(\"pe\", pe) def forward(self, x): # adds token embedding to its position embedding x = x + self.pe[:, : x.size(1)].requires_grad_(False) return self.dropout(x) P.s. This method works well for text. For images, people use relative positional embedding. Frequency is a different way of measuring horizontal stretch. With sinusoidal functions, frequency is the number of cycles that occur in $2\\pi$. For example, frequency of $sin(3x)$ is $3$, and frequency of $\\sin(x/3)$ is $1/3$.\nMulti-head attention The multi-head attention consists of multiple attention heads. Each attention head is a mechanism that assign a different amount of weight or “attention” to each element in the sequence. It does so by updating each token embedding as weighted average of all token embeddings in the sequence. Embeddings that are generated in this way are called contextualized embeddings. The weights in the weighted average are called attention weights. There are several ways to implement a self-attention layer; the most common method is scaled dot-product attention.\nScaled dot-product attention There are three main steps in this method:\nFirst, It projects each token embedding into three vectors called query, key and value. It does so by applying three independent linear projections to each token embedding. If $T \\in \\mathbb{R}^d$ is the input token embedding in $d$ dimensional space, and $W^Q \\in \\mathbb{R}^k, W^K \\in \\mathbb{R}^k$ and $W^V \\in \\mathbb{R}^v$ denote projections matrices for computing query, key and value, then the respective embedding would be $Q = TW^Q, K=TW^K$ and $V=TW^V$. Second, computes attention scores. It measures similarity of a query and a key via dot-product method, which computes very efficiently using matrix multiplications. Queries and keys which are similar will have a very large dot-product, while those that don’t share much in common will have little to no overlap. The outcome of this step is called attention scores. For a sequence with $n$ input tokens, there is a corresponding $n\\times n$ matrix of attention scores. Third it computes attention weights. Dot-products be arbitrary large and will lead to destabilizing training, therefore in this step we scale them down by dividing by $\\sqrt{dim}$, and then apply softmax to make them sum up to $1$. If we don’t scale down by $\\sqrt{dim}$ the softmax we apply might saturate early. The outcome of softmax is called attention weights. It then updates token embeddings as weighted average of value embeddings, where weights are the attention weights. We will explain the Mask (optional) in later sections when we discuss decoder.\nIn code,\ndef attention(query, key, value, mask=None, dropout=None): ''' Compute Scaled Dot Product Attention ''' d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) p_attn = scores.softmax(dim = -1) if dropout is not None: p_attn = dropout(p_attn) return torch.matmul(p_attn, value), p_attn It is beneficial to have multiple attention head and combine their embeddings. Multi-head attention jointly attend to information from different representation subspaces by concatenating them and passing them through a linear projection. Figure below, shows single attention and multi-head attention where they receive token embeddings in $d_{model}$ dimension, and outputs contextualized embeddings in $d_{model}$ dimension. Note since $d_v$ does not need to be same as $d_{model}$, we pass it throuhg a final linear layer denoted as $W^0$ to project results back to $d_{model}$ dimension. In code,\nclass MultiHeadedAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1): # Take in model size and number of heads. super(MultiHeadedAttention, self).__init__() assert d_model % h == 0 # We assume d_v always equals d_k self.d_k = d_model // h self.h = h self.linears = clones(nn.Linear(d_model, d_model), 4) self.attn = None self.dropout = nn.Dropout(p=dropout) def forward(self, query, key, value, mask=None): if mask is not None: # Same mask applied to all h heads. mask = mask.unsqueeze(1) nbatches = query.size(0) # Do all the linear projections in batch from d_model =\u003e h x d_k query, key, value = [ lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for lin, x in zip(self.linears, (query, key, value)) ] # Apply attention on all the projected vectors in batch. x, self.attn = attention( query, key, value, mask=mask, dropout=self.dropout ) # Concat using a view and apply a final linear. x = ( x.transpose(1, 2) .contiguous() .view(nbatches, -1, self.h * self.d_k) ) return self.linears[-1](x) Feed-Forward Layer This sub-layer is just a simple two layer fully connected neural network that processes embedding vector independently. This is in contrast to processing the whole sequence of embeddings as a single vector. For this reason, this layer is often referred to as position-wise feed-forward layer. Usually the hidden size of the first layer is $4d_{model}$ and a GELU activation function is mostly used. This is where most of the memorization happens, and when scaling the model this dimension usually scales up.\nDecoder The decoder’s job is to generate text sequences. Below, we see an image of the decoder block, and as we see it consists of multiple decoder layers. Every decoder layer has two multi-headed attention sub-layers, a pointwise feed-forward layer, residual connections, and layer normalization. As we see, the decoder block is capped off with a linear layer that acts as a classifier, and a softmax to get the word probabilities. The decoder is autoregressive, it begins with a start token, and it takes in a list of previous outputs as inputs, as well as the encoder outputs that contain the attention information from the input. The decoder stops decoding when it generates a token as an output.\nWhile feed-forward sub-layer behaves similarly as its counterpart in encoder, the two multi-headed attention (MHA) sub-layers are slightly different from MHA in encoder. Below, we explain the differences.\nFirst masked multi-head attention: To prevent the decoder from looking at future tokens, a look ahead mask is applied. So the decoder is only allowed to attend to earlier positions in the output sequence. The mask is added before calculating the softmax, and after scaling the scores in attention mechanism. So it computes the scaled dot-product scores between query and keys, then adds the look-ahead mask to mask future words, then calculate softmax on them. The look-ahead mask is a key-by-key square matrix where lower diagonal of it is zero, and upper diagonal is set to $-\\infty$. Once you take the softmax of the masked scores, the negative infinities get zeroed out, leaving zero attention scores for future tokens. This masking is the only difference in how the attention scores are calculated in the first multi-headed attention layer.\nSecond multi-head attention: For this layer, the encoder’s outputs are the queries and the keys, and the first multi-headed attention layer outputs are the values. This process matches the encoder’s input to the decoder’s input, allowing the decoder to decide which encoder input is relevant to put a focus on. The output of the second multi-headed attention goes through a pointwise feedforward layer for further processing.\ndef subsequent_mask(size): # Mask out subsequent positions. attn_shape = (1, size, size) subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(torch.uint8) return subsequent_mask == 0 class DecoderLayer(nn.Module): # Decoder is made of self-attn, src-attn, and feed forward (defined below)\" def __init__(self, size, self_attn, src_attn, feed_forward, dropout): super(DecoderLayer, self).__init__() self.size = size self.self_attn = self_attn self.src_attn = src_attn self.feed_forward = feed_forward self.sublayer = clones(SublayerConnection(size, dropout), 3) def forward(self, x, memory, src_mask, tgt_mask): # Follow Figure 1 (right) for connections.\" m = memory x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) return self.sublayer[2](x, self.feed_forward) class Decoder(nn.Module): # Generic N layer decoder with masking.\" def __init__(self, layer, N): super(Decoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, memory, src_mask, tgt_mask): for layer in self.layers: x = layer(x, memory, src_mask, tgt_mask) return self.norm(x) The full code is available at https://github.com/mina-ghashami/transformer-in-pytorch\nThank you If you have any questions please reach out to me:\nmina.ghashami@gmail.com\nhttps://www.linkedin.com/in/minaghashami/\nFollow me on medium for more content: https://medium.com/@mina.ghashami\n","wordCount":"2296","inLanguage":"en","datePublished":"2023-01-05T11:18:07-08:00","dateModified":"2023-01-05T11:18:07-08:00","author":{"@type":"Person","name":"Mina Ghashami"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://mina-ghashami.github.io/posts/2023-01-10-transformer/"},"publisher":{"@type":"Organization","name":"Mina Ghashami's blog","logo":{"@type":"ImageObject","url":"https://mina-ghashami.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mina-ghashami.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://mina-ghashami.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mina-ghashami.github.io/posts/ title=posts><span>posts</span></a></li><li><a href=https://mina-ghashami.github.io/aboutme/ title="about me"><span>about me</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Transformer: Concept and code from scratch</h1><div class=post-meta><span title='2023-01-05 11:18:07 -0800 -0800'>January 2023</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;2296 words&nbsp;·&nbsp;Mina Ghashami&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/2023-01-10-transformer/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#encoder>Encoder</a><ul><li><a href=#positional-embedding>Positional Embedding</a></li><li><a href=#multi-head-attention>Multi-head attention</a></li><li><a href=#feed-forward-layer>Feed-Forward Layer</a></li></ul></li><li><a href=#decoder>Decoder</a></li><li><a href=#thank-you>Thank you</a></li></ul></nav></div></details></div><div class=post-content><p>Transformers are novel neural networks that are mainly used for <em>sequence transduction</em> tasks. Sequence transduction is any task where input sequences are transformed into output sequences. Most competitive neural sequence transduction models have an encoder-decoder structure. The encoder maps an input sequence of symbol representations to a sequence of continuous representations, the decoder then generates an output sequence of symbols one element at a time. At each step the model is <em>auto-regressive</em>, consuming the previously generated symbols as additional input when generating the next. In this post, I&rsquo;ll document my learnings on main building blocks of transformer and how to implement them using PyTorch.</p><pre tabindex=0><code>import copy
import torch
import math
import torch.nn as nn
from torch.nn.functional import log_softmax, pad

class EncoderDecoder(nn.Module):
    &#34;&#34;&#34;
    A standard Encoder-Decoder architecture. 
    &#34;&#34;&#34;

    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):
        super(EncoderDecoder, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.src_embed = src_embed
        self.tgt_embed = tgt_embed
        self.generator = generator

    def forward(self, src, tgt, src_mask, tgt_mask):
        # Take in and process masked src and target sequences.
        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)

    def encode(self, src, src_mask):
        # Pass input sequence i.e. src through encoder
        return self.encoder(self.src_embed(src), src_mask)

    def decode(self, memory, src_mask, tgt, tgt_mask):
        # Memory is the query and key from encoder
        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
        
        

class Generator(nn.Module):
    # Define standard linear + softmax generation step.

    def __init__(self, d_model, vocab):
        super(Generator, self).__init__()
        self.proj = nn.Linear(d_model, vocab)

    def forward(self, x):
        return log_softmax(self.proj(x), dim=-1)
</code></pre><h2 id=encoder>Encoder<a hidden class=anchor aria-hidden=true href=#encoder>#</a></h2><p>Encoder&rsquo;s job is to convert an input sequence of tokens into a contextualized sequence of embedding vectors. By contextualized we mean the embedding of each token contains information about the context of the sequence such that the word &ldquo;apple&rdquo; as a fruit have a different embedding than &ldquo;apple&rdquo; as a company.</p><p>The encoder block is demonstrated in the figure below; it consists of multiple (historically 6) encoder layers. First encoder layer, takes the input embedding and add positional embeddings to it. Positional embeddings is shown as <font style="color: blue">$\sim$</font> symbol in the image, and we&rsquo;ll explain it soon. This only happens in the first encoder layer.
All encoder layers has two sub-layers: a multi-head self-attention mechanism, and a simple, position-wise fully connected feed-forward network. Each encoder layer receives a sequence of embeddings and feeds them through the two sublayers. The output embedding of an encoder layer has the same size as its inputs, and it is passed to the next encoder layer. Eventually all encoder layers together update the input embeddings so that they contain some contextual information from the sequence.</p><p>Each encoder layer uses skip connections, shown with <font style="color: cyan">cyan arrows</font> in the image below, and layer normalization, shows as &ldquo;add & norm&rdquo; box in the image below. Before, we dive into the encoder code, we explain each of these components.</p><table><tr><td><img src=encoder.png width=600></td><td style=width:400px;vertical-align:top><br><br><b>Skip connection</b> also known as residual connection pass a tensor to the next layer without processing and add it to the processed tensor. Skip connection is a simple yet very effective technique to mitigate the vanishing gradient problem in training deep neural networks and help them converge faster. They were first introduced in ResNet in computer vision.<p><br><br><b>Layer normalization</b> which is shown as &ldquo;add & norm&rdquo; box normalizes each input in the batch to have zero mean and unit variance.
In literature, there are two ways to place &ldquo;Add & Norm&rdquo;: <i>Post layer normalization</i> as shown in this figure where normalization happens after attention, and <i>pre layer norm</i> where &ldquo;Add & Norm&rdquo; is placed before multi-head attention. The former was used in the original transformer paper and is tricky to train from scratch as the gradients diverge. The latter however is more stable during training and is most commonly used.</p></td></tr></table><h3 id=positional-embedding>Positional Embedding<a hidden class=anchor aria-hidden=true href=#positional-embedding>#</a></h3><p>In addition to embed semantic of a token, we need to embed token&rsquo;s position in the sequence too. This is because transformer processes input tokens in parallel and not sequentially; as a result it does not carry any information on positions of tokens, so we need to embed this information into the input. Positional embeddings are identifiers that are added to tokens’ embeddings, and must satisfy two requirements:</p><ol><li>It should be the same for a position irrespective of the token in that position. So while the sequence might change, the positional embeddings must stay the same.</li><li>They should not be too large, or otherwise they will dominate semantic similarity.</li></ol><p>Based on above criteria, we can not use a non-periodic function (e.g. linear) that numbers tokens from 1 to total number of tokens. Such functions violate the second requirement above. A working option would be to use <em>sine and cosine</em> functions.
Sine and cosine periodically return a number in $(-1,1)$ range and are bounded. They are defined everywhere, and so even on very large sequences all tokens receive a positional embedding. Moreover, they have large variability even for large numbers. This is in contrast to <em>sigmoid</em> function which gets flat on large values. The disadvantage of <em>sine/cosine</em> is that it repeats same outcome for many positions. This is not desirable, so we can give our function a low frequency (so that <em>sine/cosine</em> is stretched) such that even for our biggest sequence length the numbers do not repeat. This has the benefit of the linear function while positions are bounded BUT it has the cons of having little difference between consecutive positions. We want different positions to have meaningful differences. So we use a low-frequency <em>sine</em> for first dimension of positional embeddings. For the second dimension, we use <em>cosine</em> with a higher frequency.</p><p>We repeat this for all dimensions: alternate between <em>sine</em> and <em>cosine</em> and increase frequency. In math,
$PE_{(pos, 2i)} = \sin(pos/10000^{2i/d_{model}})$ and $PE_{(pos, 2i+1)} = \cos(pos/10000^{2i/d_{model}})$ for position $pos$ and dimension $i$.</p><pre tabindex=0><code>class PositionalEncoding(nn.Module):
    # Implement the position encoding (PE) function.

    def __init__(self, d_model, dropout, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        # Compute the positional encodings once in log space.
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer(&#34;pe&#34;, pe)

    def forward(self, x):
        # adds token embedding to its position embedding
        x = x + self.pe[:, : x.size(1)].requires_grad_(False)
        return self.dropout(x)
</code></pre><p>P.s. This method works well for text. For images, people use relative positional embedding. Frequency is a different way of measuring horizontal stretch. With sinusoidal functions, <em>frequency</em> is the number of cycles that occur in $2\pi$. For example, frequency of $sin(3x)$ is $3$, and frequency of $\sin(x/3)$ is $1/3$.</p><h3 id=multi-head-attention>Multi-head attention<a hidden class=anchor aria-hidden=true href=#multi-head-attention>#</a></h3><p>The multi-head attention consists of multiple attention heads. Each attention head is a mechanism that assign a different amount of weight or &ldquo;attention&rdquo; to each element in the sequence. It does so by updating each token embedding as weighted average of all token embeddings in the sequence. Embeddings that are generated in this way are called <em>contextualized embeddings</em>. The weights in the weighted average are called <em>attention weights</em>. There are several ways to implement a self-attention layer; the most common method is <em>scaled dot-product attention</em>.</p><h4 id=scaled-dot-product-attention>Scaled dot-product attention<a hidden class=anchor aria-hidden=true href=#scaled-dot-product-attention>#</a></h4><p>There are three main steps in this method:</p><ol><li>First, It projects each token embedding into three vectors called query, key and value. It does so by applying three independent linear projections to each token embedding. If $T \in \mathbb{R}^d$ is the input token embedding in $d$ dimensional space, and $W^Q \in \mathbb{R}^k, W^K \in \mathbb{R}^k$ and $W^V \in \mathbb{R}^v$ denote projections matrices for computing query, key and value, then the respective embedding would be $Q = TW^Q, K=TW^K$ and $V=TW^V$.</li><li>Second, computes attention scores. It measures similarity of a query and a key via dot-product method, which computes very efficiently using matrix multiplications. Queries and keys which are similar will have a very large dot-product, while those that don&rsquo;t share much in common will have little to no overlap. The outcome of this step is called attention scores. For a sequence with $n$ input tokens, there is a corresponding $n\times n$ matrix of attention scores.</li><li>Third it computes attention weights. Dot-products be arbitrary large and will lead to destabilizing training, therefore in this step we scale them down by dividing by $\sqrt{dim}$, and then apply softmax to make them sum up to $1$. If we don&rsquo;t scale down by $\sqrt{dim}$ the softmax we apply might saturate early. The outcome of softmax is called <em>attention weights</em>.</li><li>It then updates token embeddings as weighted average of value embeddings, where weights are the attention weights.</li></ol><p><img alt="single head attention" height=150 src=/posts/2023-01-10-transformer/attn_steps_hu6c2234164d4bbf7e3688165a8b7cd2b5_34668_0x150_resize_box_3.png title="Attention steps" width=838></p><p>We will explain the <em>Mask (optional)</em> in later sections when we discuss decoder.</p><p>In code,</p><pre tabindex=0><code>def attention(query, key, value, mask=None, dropout=None):
    &#39;&#39;&#39;
     Compute Scaled Dot Product Attention
    &#39;&#39;&#39;

    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    p_attn = scores.softmax(dim = -1)
    if dropout is not None:
        p_attn = dropout(p_attn)
    return torch.matmul(p_attn, value), p_attn
</code></pre><p>It is beneficial to have multiple attention head and combine their embeddings. Multi-head attention jointly attend to information from different representation subspaces by concatenating them and passing them through a linear projection. Figure below, shows single attention and multi-head attention where they receive token embeddings in $d_{model}$ dimension, and outputs contextualized embeddings in $d_{model}$ dimension. Note since $d_v$ does not need to be same as $d_{model}$, we pass it throuhg a final linear layer denoted as $W^0$ to project results back to $d_{model}$ dimension.
<img alt="Multi head attention" height=252 src=/posts/2023-01-10-transformer/attn_both_hu18f2ccbc43c5a217b9efbc51cbd0014c_294846_600x0_resize_box_3.png width=600></p><p>In code,</p><pre tabindex=0><code>class MultiHeadedAttention(nn.Module):
    def __init__(self, h, d_model, dropout=0.1):
        # Take in model size and number of heads.
        
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0
        # We assume d_v always equals d_k
        self.d_k = d_model // h
        self.h = h
        self.linears = clones(nn.Linear(d_model, d_model), 4)
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, query, key, value, mask=None):
        if mask is not None:
            # Same mask applied to all h heads.
            mask = mask.unsqueeze(1)
        nbatches = query.size(0)

        # Do all the linear projections in batch from d_model =&gt; h x d_k
        query, key, value = [
            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
            for lin, x in zip(self.linears, (query, key, value))
        ]

        # Apply attention on all the projected vectors in batch.
        x, self.attn = attention(
            query, key, value, mask=mask, dropout=self.dropout
        )

        # Concat using a view and apply a final linear.
        x = (
            x.transpose(1, 2)
            .contiguous()
            .view(nbatches, -1, self.h * self.d_k)
        )
        return self.linears[-1](x)
</code></pre><h3 id=feed-forward-layer>Feed-Forward Layer<a hidden class=anchor aria-hidden=true href=#feed-forward-layer>#</a></h3><p>This sub-layer is just a simple two layer fully connected neural network that processes embedding vector independently. This is in contrast to processing the whole sequence of embeddings as a single vector. For this reason, this layer is often referred to as <em>position-wise feed-forward layer</em>. Usually the hidden size of the first layer is $4d_{model}$ and a GELU activation function is mostly used. This is where most of the memorization happens, and when scaling the model this dimension usually scales up.</p><h2 id=decoder>Decoder<a hidden class=anchor aria-hidden=true href=#decoder>#</a></h2><p>The decoder&rsquo;s job is to generate text sequences. Below, we see an image of the decoder block, and as we see it consists of multiple decoder layers.
Every decoder layer has two multi-headed attention sub-layers, a pointwise feed-forward layer, residual connections, and layer normalization.
As we see, the decoder block is capped off with a linear layer that acts as a classifier, and a softmax to get the word probabilities.
The decoder is autoregressive, it begins with a start token, and it takes in a list of previous outputs as inputs, as well as the encoder outputs that contain the attention information from the input. The decoder stops decoding when it generates a token as an output.</p><p><img alt="Scenario 1: decoder architecture" class=left height=700 src=/posts/2023-01-10-transformer/decoder_hud1756a4db56c2a44c93d541ce7bfed4f_90532_0x700_resize_q10_box_3.png width=294></p><p>While feed-forward sub-layer behaves similarly as its counterpart in encoder, the two multi-headed attention (MHA) sub-layers are slightly different from MHA in encoder. Below, we explain the differences.</p><p><em>First masked multi-head attention</em>: To prevent the decoder from looking at future tokens, a look ahead mask is applied. So the decoder is only allowed to attend to earlier positions in the output sequence. The mask is added before calculating the softmax, and after scaling the scores in attention mechanism. So it computes the scaled dot-product scores between query and keys, then adds the look-ahead mask to mask future words, then calculate softmax on them. The look-ahead mask is a key-by-key square matrix where lower diagonal of it is zero, and upper diagonal is set to $-\infty$.
Once you take the softmax of the masked scores, the negative infinities get zeroed out, leaving zero attention scores for future tokens.
This masking is the only difference in how the attention scores are calculated in the first multi-headed attention layer.</p><p><em>Second multi-head attention</em>: For this layer, the encoder’s outputs are the queries and the keys, and the first multi-headed attention layer outputs are the values. This process matches the encoder’s input to the decoder’s input, allowing the decoder to decide which encoder input is relevant to put a focus on. The output of the second multi-headed attention goes through a pointwise feedforward layer for further processing.</p><pre tabindex=0><code>def subsequent_mask(size):
    # Mask out subsequent positions.
    attn_shape = (1, size, size)
    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(torch.uint8)
    return subsequent_mask == 0

class DecoderLayer(nn.Module):
    # Decoder is made of self-attn, src-attn, and feed forward (defined below)&#34;

    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):
        super(DecoderLayer, self).__init__()
        self.size = size
        self.self_attn = self_attn
        self.src_attn = src_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 3)

    def forward(self, x, memory, src_mask, tgt_mask):
        # Follow Figure 1 (right) for connections.&#34;
        m = memory
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))
        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))
        return self.sublayer[2](x, self.feed_forward)


class Decoder(nn.Module):
    # Generic N layer decoder with masking.&#34;

    def __init__(self, layer, N):
        super(Decoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)

    def forward(self, x, memory, src_mask, tgt_mask):
        for layer in self.layers:
            x = layer(x, memory, src_mask, tgt_mask)
        return self.norm(x)
</code></pre><p>The full code is available at <a href=https://github.com/mina-ghashami/transformer-in-pytorch>https://github.com/mina-ghashami/transformer-in-pytorch</a></p><h2 id=thank-you>Thank you<a hidden class=anchor aria-hidden=true href=#thank-you>#</a></h2><p>If you have any questions please reach out to me:</p><p><a href=mailto:mina.ghashami@gmail.com>mina.ghashami@gmail.com</a></p><p><a href=https://www.linkedin.com/in/minaghashami/>https://www.linkedin.com/in/minaghashami/</a></p><p>Follow me on medium for more content: <a href=https://medium.com/@mina.ghashami>https://medium.com/@mina.ghashami</a></p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://mina-ghashami.github.io/posts/2022-09-16-over-param_convergence/><span class=title>Next »</span><br><span>Convergence of gradient descent in over-parameterized networks</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://mina-ghashami.github.io/>Mina Ghashami's blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>